import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from bs4 import BeautifulSoup

# دانلود کلمات توقف از کتابخانه nltk
nltk.download('stopwords')

# تعریف مجموعه کلمات توقف
stop_words = set(stopwords.words('english'))

# خواندن فایل CSV
file_path = '/content/Spam_Detection (1).csv'  # جایگزین کنید با نام فایل آپلود شده
df = pd.read_csv(file_path)

# نمایش چند سطر اول دیتافریم
df.head()


  # تعریف تابع برای حذف ایموجی‌ها
def remove_emojis(text):
    emoji_pattern = re.compile(
        "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F700-\U0001F77F"  # alchemical symbols
        u"\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
        u"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
        u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
        u"\U0001FA00-\U0001FA6F"  # Chess Symbols
        u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
        u"\U00002702-\U000027B0"  # Dingbats
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

# تعریف تابع پیش‌پردازش متن
def preprocess_text(text):
    # حذف تگ‌های HTML
    text = BeautifulSoup(text, "html.parser").get_text()

    # حذف URL‌ها
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # حذف علائم نگارشی
    text = re.sub(r'\W', ' ', text)

    # حذف ایموجی‌ها
    text = remove_emojis(text)

    # تبدیل متن به حروف کوچک
    text = text.lower()

    # حذف کلمات توقف
    text = ' '.join([word for word in text.split() if word not in stop_words])

    return text

# اعمال تابع پیش‌پردازش به ستون 'Mail' در دیتافریم
df['Processed_Mail'] = df['Mail'].apply(preprocess_text)

# نمایش چند سطر اول دیتافریم پردازش‌شده
df[['Label', 'Mail', 'Processed_Mail']].head()




  import pandas as pd
import matplotlib.pyplot as plt



# شمارش نمونه‌ها در هر کلاس
class_counts = df['Label'].value_counts()

# نمایش تعداد نمونه‌ها در هر کلاس
print("تعداد نمونه‌ها در هر کلاس:")
print(class_counts)

# محاسبه نسبت‌ها
total_samples = class_counts.sum()
ratios = class_counts / total_samples

# نمایش نسبت‌ها
print("\nنسبت نمونه‌ها در هر کلاس:")
print(ratios)

# نمایش گرافیکی توزیع داده‌ها
plt.figure(figsize=(8, 6))
class_counts.plot(kind='bar', color=['skyblue', 'salmon'])
plt.title('توزیع نمونه‌ها در هر کلاس')
plt.xlabel('کلاس')
plt.ylabel('تعداد نمونه‌ها')
plt.xticks(rotation=0)
plt.show()



  import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

# فرض می‌کنیم df داده‌های شماست و 'Text' نام ستون متنی و 'Label' نام ستون کلاس‌هاست
# بارگذاری داده‌ها
# df = pd.read_csv('path_to_your_email_data.csv')

# جدا کردن ویژگی‌ها و برچسب‌ها
X = df['Processed_Mail']  # ایمیل‌ها به عنوان ویژگی
y = df['Label']  # برچسب‌ها

# تبدیل متن به ویژگی‌های عددی با استفاده از TF-IDF
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_tfidf = vectorizer.fit_transform(X)

# تقسیم داده‌ها به داده‌های آموزشی و آزمایشی
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)

# ایجاد شیء SMOTE
smote = SMOTE(random_state=42)

# اعمال SMOTE به داده‌های آموزشی
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# نمایش توزیع نمونه‌ها در داده‌های بالانس شده
class_counts_resampled = pd.Series(y_resampled).value_counts()
print("\nتعداد نمونه‌ها در هر کلاس پس از بالانس:")
print(class_counts_resampled)

# نمایش گرافیکی توزیع داده‌های بالانس شده
plt.figure(figsize=(8, 6))
class_counts_resampled.plot(kind='bar', color=['skyblue', 'salmon'])
plt.title('توزیع نمونه‌ها در هر کلاس پس از بالانس')
plt.xlabel('کلاس')
plt.ylabel('تعداد نمونه‌ها')
plt.xticks(rotation=0)
plt.show()



from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter



# نمایش توزیع نمونه‌ها در هر کلاس (برای درک بهتر)
import matplotlib.pyplot as plt

# تعداد نمونه‌های هر کلاس
class_counts = Counter(y_train)

# رسم نمودار توزیع
plt.figure(figsize=(8, 6))
plt.bar(class_counts.keys(), class_counts.values(), color=['blue', 'orange'])
plt.xlabel('کلاس')
plt.ylabel('تعداد نمونه‌ها')
plt.title('توزیع نمونه‌ها در هر کلاس پس از بالانس کردن')
plt.xticks([0, 1], ['ham', 'spam'])
plt.show()



!pip install nltk



import pandas as pd
import nltk
from nltk.tokenize import word_tokenize

# دانلود منابع توکنیزیشن nltk
nltk.download('punkt')


# توکنیزیشن متن
def tokenize_text(text):
    return word_tokenize(text)

# اعمال توکنیزیشن بر روی ستون ویژگی‌ها
df['Tokenized_Mail'] = df['Processed_Mail'].apply(tokenize_text)

# نمایش نمونه‌ای از توکنیزه شده
print("نمونه‌ای از داده‌های توکنیزه شده:")
print(df[['Processed_Mail', 'Tokenized_Mail']].head())





import pandas as pd
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# دانلود منابع توکنیزیشن
nltk.download('punkt')


# تبدیل برچسب‌های متنی به عددی
df['Label'] = df['Label'].map({'ham': 0, 'spam': 1})

# توکنیزیشن متن
df['Tokenized_Mail'] = df['Processed_Mail'].apply(lambda x: word_tokenize(x.lower()))

# ایجاد توکنایزر
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['Tokenized_Mail'])

# تبدیل توکن‌ها به دنباله‌های عددی
X = tokenizer.texts_to_sequences(df['Tokenized_Mail'])
print(X)
# تنظیم طول دنباله‌ها
max_sequence_length = max(len(x) for x in X)
X = pad_sequences(X, maxlen=max_sequence_length)

# ویژگی‌ها و برچسب‌ها
y = df['Label'].values

# تقسیم داده‌ها به مجموعه‌های آموزشی و تست
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# نمایش ابعاد داده‌های آماده شده
print(f"ابعاد داده‌های آموزشی: {X_train.shape}")
print(f"ابعاد داده‌های تست: {X_test.shape}")



from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# آماده‌سازی داده‌ها
texts = df['Processed_Mail'].values
labels = df['Label'].values

# توکن‌سازی متون
tokenizer = Tokenizer(num_words=10000)  # تعداد کلمات در دیکشنری
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# پدینگ توالی‌ها
X = pad_sequences(sequences, maxlen=100)  # طول توالی‌ها به 100 محدود می‌شود

# تبدیل برچسب‌ها به آرایه NumPy
y = np.array(labels)





from sklearn.model_selection import train_test_split

# تقسیم داده‌ها
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, SimpleRNN, Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import numpy as np

# بارگذاری داده‌های IMDb
max_features = 10000  # تعداد بیشترین کلمات
max_len = 100  # حداکثر طول دنباله

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)

# پد کردن دنباله‌ها به طول ثابت
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

# تنظیمات Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=2)

# تعریف مدل LSTM
def create_lstm_model():
    model = Sequential()
    model.add(Embedding(input_dim=max_features, output_dim=128, input_length=max_len))
    model.add(LSTM(64))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# تعریف مدل RNN
def create_rnn_model():
    model = Sequential()
    model.add(Embedding(input_dim=max_features, output_dim=128, input_length=max_len))
    model.add(SimpleRNN(64))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# تعریف مدل GRU
def create_gru_model():
    model = Sequential()
    model.add(Embedding(input_dim=max_features, output_dim=128, input_length=max_len))
    model.add(GRU(64))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# آموزش و ارزیابی مدل LSTM
lstm_model = create_lstm_model()
lstm_history = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping])

# ارزیابی مدل LSTM
lstm_final_scores = lstm_model.evaluate(X_test, y_test, verbose=0)
print(f"LSTM Test accuracy: {lstm_final_scores[1] * 100:.2f}%")

# آموزش و ارزیابی مدل RNN
rnn_model = create_rnn_model()
rnn_history = rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping])

# ارزیابی مدل RNN
rnn_final_scores = rnn_model.evaluate(X_test, y_test, verbose=0)
print(f"RNN Test accuracy: {rnn_final_scores[1] * 100:.2f}%")

# آموزش و ارزیابی مدل GRU
gru_model = create_gru_model()
gru_history = gru_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping])

# ارزیابی مدل GRU
gru_final_scores = gru_model.evaluate(X_test, y_test, verbose=0)
print(f"GRU Test accuracy: {gru_final_scores[1] * 100:.2f}%")

# ترسیم نمودار دقت و خطا برای مدل‌ها

plt.figure(figsize=(12, 18))

# LSTM
plt.subplot(3, 2, 1)
plt.plot(lstm_history.history['accuracy'], label='LSTM Train Accuracy')
plt.plot(lstm_history.history['val_accuracy'], label='LSTM Validation Accuracy')
plt.title('LSTM Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(3, 2, 2)
plt.plot(lstm_history.history['loss'], label='LSTM Train Loss')
plt.plot(lstm_history.history['val_loss'], label='LSTM Validation Loss')
plt.title('LSTM Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# RNN
plt.subplot(3, 2, 3)
plt.plot(rnn_history.history['accuracy'], label='RNN Train Accuracy')
plt.plot(rnn_history.history['val_accuracy'], label='RNN Validation Accuracy')
plt.title('RNN Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(3, 2, 4)
plt.plot(rnn_history.history['loss'], label='RNN Train Loss')
plt.plot(rnn_history.history['val_loss'], label='RNN Validation Loss')
plt.title('RNN Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# GRU
plt.subplot(3, 2, 5)
plt.plot(gru_history.history['accuracy'], label='GRU Train Accuracy')
plt.plot(gru_history.history['val_accuracy'], label='GRU Validation Accuracy')
plt.title('GRU Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(3, 2, 6)
plt.plot(gru_history.history['loss'], label='GRU Train Loss')
plt.plot(gru_history.history['val_loss'], label='GRU Validation Loss')
plt.title('GRU Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# پیش‌بینی بر روی داده‌های تست
y_pred_lstm = lstm_model.predict(X_test)
y_pred_rnn = rnn_model.predict(X_test)
y_pred_gru = gru_model.predict(X_test)

# تبدیل پیش‌بینی‌های احتمالاتی به برچسب‌های کلاس
y_pred_lstm = (y_pred_lstm > 0.5).astype(int)
y_pred_rnn = (y_pred_rnn > 0.5).astype(int)
y_pred_gru = (y_pred_gru > 0.5).astype(int)

# محاسبه precision، recall و F1-score
print("\nLSTM Classification Report:")
print(classification_report(y_test, y_pred_lstm, target_names=['Class 0', 'Class 1']))

print("\nRNN Classification Report:")
print(classification_report(y_test, y_pred_rnn, target_names=['Class 0', 'Class 1']))

print("\nGRU Classification Report:")
print(classification_report(y_test, y_pred_gru, target_names=['Class 0', 'Class 1']))




from tensorflow.keras.optimizers import Adam, SGD, RMSprop

# تعریف بهینه‌سازها و نرخ‌های یادگیری مختلف
optimizers = {
    'Adam': Adam,
    'SGD': SGD,
    'RMSprop': RMSprop
}

learning_rates = [0.001, 0.01, 0.1]




def create_model(optimizer, learning_rate):
    model = Sequential()
    model.add(Embedding(input_dim=max_features, output_dim=128, input_length=max_len))
    model.add(GRU(64))
    model.add(Dense(1, activation='sigmoid'))

    opt = optimizer(learning_rate=learning_rate)
    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])

    return model


results = []

for opt_name, optimizer in optimizers.items():
    for lr in learning_rates:
        print(f"\nTraining with {opt_name} optimizer and learning rate {lr}")

        model = create_model(optimizer, lr)
        history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping])

        scores = model.evaluate(X_test, y_test, verbose=0)
        results.append((opt_name, lr, scores[1] * 100))
        print(f"Test accuracy with {opt_name} optimizer and learning rate {lr}: {scores[1] * 100:.2f}%")

# پیدا کردن بهترین ترکیب
best_result = max(results, key=lambda x: x[2])
print(f"\nBest optimizer: {best_result[0]} with learning rate: {best_result[1]}")
print(f"Test accuracy: {best_result[2]:.2f}%")


import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import KFold
import numpy as np


from tensorflow.keras.layers import LSTM

def create_lstm_model():
    model = Sequential()
    model.add(Embedding(input_dim=max_features, output_dim=128, input_length=max_len))
    model.add(LSTM(64))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model


from tensorflow.keras.layers import SimpleRNN

def create_rnn_model():
    model = Sequential()
    model.add(Embedding(input_dim=max_features, output_dim=128, input_length=max_len))
    model.add(SimpleRNN(64))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model



from tensorflow.keras.layers import GRU

def create_gru_model():
    model = Sequential()
    model.add(Embedding(input_dim=max_features, output_dim=128, input_length=max_len))
    model.add(GRU(64))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model




# آموزش و ارزیابی مدل LSTM
lstm_model = create_lstm_model()
lstm_history = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping])

# ارزیابی مدل LSTM
lstm_final_scores = lstm_model.evaluate(X_test, y_test, verbose=0)
print(f"LSTM Test accuracy: {lstm_final_scores[1] * 100:.2f}%")



import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import KFold
import numpy as np

# بارگذاری داده‌های IMDb
max_features = 10000  # تعداد بیشترین کلمات
max_len = 100  # حداکثر طول دنباله

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)

# پد کردن دنباله‌ها به طول ثابت
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

# تعریف تابعی برای ساخت مدل
def create_model():
    model = Sequential()
    model.add(Embedding(input_dim=max_features, output_dim=128, input_length=max_len))
    model.add(GRU(64))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# تنظیمات Cross-Validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
cvscores = []

# تنظیمات Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=2)

# انجام Cross-Validation
for train, val in kfold.split(X_train, y_train):
    model = create_model()
    history = model.fit(X_train[train], y_train[train], epochs=10, batch_size=32,
                        validation_data=(X_train[val], y_train[val]), verbose=1,
                        callbacks=[early_stopping])

    # ارزیابی مدل بر روی داده‌های اعتبارسنجی
    scores = model.evaluate(X_train[val], y_train[val], verbose=0)
    print(f"Accuracy: {scores[1] * 100:.2f}%")
    cvscores.append(scores[1] * 100)

print(f"Mean accuracy: {np.mean(cvscores):.2f}% (+/- {np.std(cvscores):.2f}%)")

# آموزش نهایی مدل بر روی تمام داده‌های آموزشی
final_model = create_model()
final_history = final_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping])

# ارزیابی نهایی مدل بر روی داده‌های تست
final_scores = final_model.evaluate(X_test, y_test, verbose=0)
print(f"Final test accuracy: {final_scores[1] * 100:.2f}%")

# ترسیم نمودار دقت
plt.plot(final_history.history['accuracy'], label='Train Accuracy')
plt.plot(final_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# ترسیم نمودار خطا
plt.plot(final_history.history['loss'], label='Train Loss')
plt.plot(final_history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()



import matplotlib.pyplot as plt

# ترسیم نمودار دقت
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# ترسیم نمودار خطا
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()




